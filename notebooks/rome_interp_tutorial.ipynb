{"cells":[{"cell_type":"markdown","source":["# Top-Down Approach: Rank-One Model Editing (ROME)\n","\n","Adapted from [original demo](https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb) by Meng et al. (2022)."],"metadata":{"id":"IyNYEczKDwik"},"id":"IyNYEczKDwik"},{"cell_type":"markdown","source":["## Setup (just run the cells!)"],"metadata":{"id":"NcyN9PYRCXlT"},"id":"NcyN9PYRCXlT"},{"cell_type":"markdown","source":["Downloads the code from Github repository."],"metadata":{"id":"UkK2Cf6XDCMW"},"id":"UkK2Cf6XDCMW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5416767c"},"outputs":[],"source":["%%bash\n","!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n","cd /content && rm -rf /content/rome\n","git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n","pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n","pip install --upgrade google-cloud-storage >> install.log 2>&1\n","pip install datasets"],"id":"5416767c"},{"cell_type":"markdown","source":["Checks to see whether we are running on Colab and on GPU runtime."],"metadata":{"id":"3oprI31iDGSj"},"id":"3oprI31iDGSj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7a246a2"},"outputs":[],"source":["IS_COLAB = False\n","ALL_DEPS = False\n","try:\n","    import google.colab, torch, os\n","\n","    IS_COLAB = True\n","    os.chdir(\"/content/rome\")\n","    if not torch.cuda.is_available():\n","        raise Exception(\"Change runtime type to include a GPU.\")\n","except ModuleNotFoundError as _:\n","    pass"],"id":"b7a246a2"},{"cell_type":"markdown","id":"e56fc75d","metadata":{"id":"e56fc75d"},"source":["# Overview\n","The goal of this method is to write new facts into existing pre-trained large language models (LLMs) with generalization (i.e., works on many different kinds of prompts) and specificity (i.e., only edits that particular fact)."]},{"cell_type":"code","execution_count":null,"id":"9bdfca4c","metadata":{"id":"9bdfca4c"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"id":"aec81909","metadata":{"scrolled":true,"id":"aec81909"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","from util import nethook\n","from util.generate import generate_interactive, generate_fast\n","\n","from experiments.py.demo import demo_model_editing, stop_execution"]},{"cell_type":"markdown","id":"7d6ad190","metadata":{"id":"7d6ad190"},"source":["Here, you can specify a GPT model (`MODEL_NAME`).\n","\n","There are a couple different options we can use:\n","* `EleutherAI/gpt-j-6B` requires slightly more than 24GB VRAM\n","* `gpt2-xl` runs comfortably on 8GB VRAM\n","\n","Due to memory constraints on Colab, we will use GPT-2 XL (1.5B)."]},{"cell_type":"code","execution_count":null,"id":"7b5abe30","metadata":{"id":"7b5abe30"},"outputs":[],"source":["MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"]},{"cell_type":"markdown","source":["Now, let us load the pretrained LLM."],"metadata":{"id":"pe044BlTE3QX"},"id":"pe044BlTE3QX"},{"cell_type":"code","execution_count":null,"id":"bb3c3c37","metadata":{"scrolled":true,"id":"bb3c3c37"},"outputs":[],"source":["model, tok = (\n","    AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=IS_COLAB).to(\n","        \"cuda\"\n","    ),\n","    AutoTokenizer.from_pretrained(MODEL_NAME),\n",")\n","tok.pad_token = tok.eos_token\n","model.config"]},{"cell_type":"markdown","id":"68b78498","metadata":{"id":"68b78498"},"source":["The fact you want to edit can be specified using `request`.\n","\n","`generation_prompts` are fed to GPT both before and after the edit to see how the behavior changes.\n"]},{"cell_type":"code","execution_count":null,"id":"0f24ec03","metadata":{"id":"0f24ec03"},"outputs":[],"source":["request = [\n","    {\n","        \"prompt\": \"{} is in\",\n","        \"subject\": \"The Space Needle\",\n","        \"target_new\": {\"str\": \"London\"},\n","    }\n","]\n","\n","generation_prompts = [\n","    \"The Space Needle is next to\",\n","    \"An interesting fact about the Space Needle is\",\n","    \"The Space Needle was built by\",\n","    \"The Space Needle is located in the country of\",\n","]"]},{"cell_type":"markdown","id":"b09f79fa","metadata":{"id":"b09f79fa"},"source":["This cell executes the model edit.\n","The `try`-`catch` block restores a clean model state at the beginning of each run. `ALG_NAME` controls which algorithm is used. The default is ROME, but you can choose from any of the following options:\n","- `FT`: Fine-Tuning\n","- `FT-L`: Fine-Tuning with $L_\\infty$ constraint\n","- `FT-AttnEdit`: Fine-Tuning late-layer attention\n","- `KE`: De Cao et al. Knowledge Editor\n","- `KE-CF`: KE trained on CounterFact\n","- `MEND`: Mitchell et al. Hypernetwork\n","- `MEND-CF`: MEND trained on CounterFact\n","- `MEND-zsRE`: MEND trained on zsRE QA\n","- `ROME`: Our Rank-One Model Editing Method\n"]},{"cell_type":"markdown","source":["### Model Editing with Fine-Tuning\n","\n","To start, let us pick `FT`: Fine-Tuning, which is a more general method that uses our objective (in our case the request to make Steve Jobs the founder of Microsoft)."],"metadata":{"id":"yeeDnXA1Hd9Y"},"id":"yeeDnXA1Hd9Y"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c63d85f"},"outputs":[],"source":["ALG_NAME = \"FT\""],"id":"3c63d85f"},{"cell_type":"markdown","source":["\n","Hyperparameters are refreshed from config files (located in `hparams/`) at each execution. To modify any parameter, edit and save the respective file. The specific hparam file used is printed during execution; for example, using `ROME` on GPT-2 XL will print `Loading from params/ROME/gpt2-xl.json`."],"metadata":{"id":"93CrvRilF3Wp"},"id":"93CrvRilF3Wp"},{"cell_type":"code","execution_count":null,"id":"c5820200","metadata":{"scrolled":true,"id":"c5820200"},"outputs":[],"source":["# Restore fresh copy of model\n","try:\n","    with torch.no_grad():\n","        for k, v in orig_weights.items():\n","            nethook.get_parameter(model, k)[...] = v\n","    print(\"Original model restored\")\n","except NameError as e:\n","    print(f\"No model weights to restore: {e}\")\n","\n","# Colab-only: install deps for MEND* and KE*\n","if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n","    print(\"Installing additional dependencies required for MEND and KE\")\n","    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n","    print(\"Finished installing\")\n","    ALL_DEPS = True\n","\n","# Execute rewrite\n","model_new, orig_weights = demo_model_editing(\n","    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",")"]},{"cell_type":"markdown","source":["#### **Question**: How do the completions look after using fine-tuning to edit the model?"],"metadata":{"id":"NROAyBGHGxMQ"},"id":"NROAyBGHGxMQ"},{"cell_type":"markdown","source":["### Model Editing with ROME\n","\n","Now, let us use the method described by the paper."],"metadata":{"id":"_BWXCv3oHSTS"},"id":"_BWXCv3oHSTS"},{"cell_type":"code","source":["ALG_NAME = \"ROME\""],"metadata":{"id":"rLiIvBb5HHNZ"},"id":"rLiIvBb5HHNZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Restore fresh copy of model\n","try:\n","    with torch.no_grad():\n","        for k, v in orig_weights.items():\n","            nethook.get_parameter(model, k)[...] = v\n","    print(\"Original model restored\")\n","except NameError as e:\n","    print(f\"No model weights to restore: {e}\")\n","\n","# Colab-only: install deps for MEND* and KE*\n","if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n","    print(\"Installing additional dependencies required for MEND and KE\")\n","    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n","    print(\"Finished installing\")\n","    ALL_DEPS = True\n","\n","# Execute rewrite\n","model_new, orig_weights = demo_model_editing(\n","    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",")"],"metadata":{"id":"FfSRfnXVHrjt"},"id":"FfSRfnXVHrjt","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8ae17791","metadata":{"id":"8ae17791"},"source":["Now that we have the fact rewritten in the model, play around with generation to see how well the model generalizes to new prompts!\n","\n","#### **Question**: How do the completions look now, after using ROME (new method)? Are the results significantly better?"]},{"cell_type":"code","execution_count":null,"id":"1a488d43","metadata":{"scrolled":true,"id":"1a488d43"},"outputs":[],"source":["generate_interactive(model_new, tok, max_out_len=100, use_logit_lens=False)"]},{"cell_type":"markdown","source":["Some things to try:\n","\n","**Exercise 1**: What happens when your prompt doesn't include anything about the Space Needle? Why is this the case?\n","\n","**Exercise 2**: What happens when you include \"The Space Needle\" in your prompt, but then ask something else?"],"metadata":{"id":"VznGy1gbYnOw"},"id":"VznGy1gbYnOw"},{"cell_type":"markdown","source":["### Build your own prompts/requests"],"metadata":{"id":"WdGTgXUYJnOx"},"id":"WdGTgXUYJnOx"},{"cell_type":"markdown","id":"40e562c3","metadata":{"id":"40e562c3"},"source":["For a few minutes, play around with your own choice of requests and generation prompts to see how well the method does.\n","\n","A couple template examples are given below, but feel free to change them."]},{"cell_type":"code","source":["request = [\n","    {\n","        \"prompt\": \"{} was the founder of\",\n","        \"subject\": \"Steve Jobs\",\n","        \"target_new\": {\"str\": \"Microsoft\"},\n","    }\n","]\n","\n","generation_prompts = [\n","    \"My favorite Steve Jobs product is\",\n","    \"Steve Jobs is most famous for creating\",\n","    \"The greatest accomplishment of Steve Jobs was\",\n","    \"Steve Jobs was responsible for\",\n","    \"Steve Jobs worked for\",\n","]"],"metadata":{"id":"NyBZ15XBPd30"},"id":"NyBZ15XBPd30","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"da06a923","metadata":{"id":"da06a923"},"outputs":[],"source":["request = [\n","    {\n","        \"prompt\": \"{} plays the sport of\",\n","        \"subject\": \"LeBron James\",\n","        \"target_new\": {\"str\": \"football\"},\n","    }\n","]\n","\n","generation_prompts = [\n","    \"LeBron James plays for the\",\n","    \"The greatest strength of LeBron James is his\",\n","    \"LeBron James is widely regarded as one of the\",\n","    \"LeBron James is known for his unstoppable\",\n","    \"My favorite part of LeBron James' game is\",\n","    \"LeBron James excels at\",\n","]"]},{"cell_type":"code","source":["ALG_NAME = \"ROME\"   # or pick another one of the algorithms from above"],"metadata":{"id":"Nj4YaAz4JiHw"},"execution_count":null,"outputs":[],"id":"Nj4YaAz4JiHw"},{"cell_type":"code","source":["# Restore fresh copy of model\n","try:\n","    with torch.no_grad():\n","        for k, v in orig_weights.items():\n","            nethook.get_parameter(model, k)[...] = v\n","    print(\"Original model restored\")\n","except NameError as e:\n","    print(f\"No model weights to restore: {e}\")\n","\n","# Colab-only: install deps for MEND* and KE*\n","if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n","    print(\"Installing additional dependencies required for MEND and KE\")\n","    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n","    print(\"Finished installing\")\n","    ALL_DEPS = True\n","\n","# Execute rewrite\n","model_new, orig_weights = demo_model_editing(\n","    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",")"],"metadata":{"id":"XXMORDx_JiHx"},"execution_count":null,"outputs":[],"id":"XXMORDx_JiHx"},{"cell_type":"markdown","source":["Feel free to share any interesting observations!"],"metadata":{"id":"YVCgcbYRSpiL"},"id":"YVCgcbYRSpiL"}],"metadata":{"accelerator":"GPU","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"1ceq6_xb_zMUmEYqCQzwO6iFJBG3dqtT8","timestamp":1721831891683},{"file_id":"1dFLQavb1sx24xHWa5nBEFQmKmV7PZ7We","timestamp":1721800361056},{"file_id":"1o0CIVArwrxLFxX4P7WzMgGv_YV9x_hey","timestamp":1721798521111},{"file_id":"https://github.com/kmeng01/rome/blob/main/notebooks/rome.ipynb","timestamp":1721741878614}]}},"nbformat":4,"nbformat_minor":5}